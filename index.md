## LARGE-SCALE DEEP LEARNING: TRENDS and APPLICATIONS
### with Invited Guest Speakers from Samsung â€“ ISC 2018

Speakers: Rangan Sukumar, Aaron Vose, Jacob Balma, Hars Vardhan, Schuyler Cullen, and Vish Rajalingam

### Abstract
Deep learning adoption in the industry is a journey that begins with identifying appropriate datasets and use-cases, investing in curation and creation of appropriate labels that map to monetizable business problems, building a team of data and machine learning scientists that are capable of designing algorithms that train fast and accurate models, and supporting the team with a performant and productive compute infrastructure.

This workshop brings together experts and practitioners that are leveraging supercomputing principles and best practices for real-world in-production use-cases to present their experience and valuable lessons. The workshop will offer a platform for exposition of how deep learning use-cases in industry are evolving, open-challenges in training deep networks at scale, the state-of-the-practice in hyper-parameter optimization, shortcomings of existing datasets and methods, and emphasize system design choices which maximize both productivity and performance.

### Workshop Outline
Cray is joined by Samsung in the following series of talks including invited industry experts in the fields of deep learning, machine learning, supercomputing, and autonomous driving. Each session will conclude with an open-forum for questions:

- Talk: Achieving Deep Learning Maturity in the Enterprise
Rangan Sukumar, Cray Inc. (1.5h)
Presents techniques and best practices for effective and efficient utilization of machine learning.
Covers the journey of deep learning for real-world enterprise problems from beginning to end.
As the first talk in the series, this includes an introduction to DL and has a big-picture perspective.

- Talk: Is Deep Learning a High-Performance Computing Problem?
Jacob Balma, Cray Inc. (1.5h)
Presents deep learning as a classic high-performance computing problem which demands:
1. large compute capacity in terms of FLOPs as well as memory capacity and bandwidth,
2. a performant node interconnect for fast exchange of gradients and model parameters, and
3. parallel IO and storage with sufficient bandwidth to keep all this compute fed at scale.

- Talk: Neural Network Hyperparameter Optimization: State-of-the-Practice
Aaron Vose, Cray Inc. (1.5h)
Presents a realistic neural network training workflow with hyperparameter optimization (HPO).
Covers the exploration and optimization of neural network topologies and training regimes.
Includes basic and advanced techniques for using HPO to increase DL practitioner productivity.

- Customer Talk: Real-World Use-Cases Beyond ImageNet: Gaps and Bottlenecks
Hars Vardhan, Schuyler Cullen, and Vish Rajalingam, Samsung Inc. (1.5h)
